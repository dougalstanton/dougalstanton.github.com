<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: programming | Looking Out To Sea]]></title>
  <link href="http://dougalstanton.github.com/blog/categories/programming/atom.xml" rel="self"/>
  <link href="http://dougalstanton.github.com/"/>
  <updated>2013-02-22T13:26:14+00:00</updated>
  <id>http://dougalstanton.github.com/</id>
  <author>
    <name><![CDATA[Dougal Stanton]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Octopress uses a non-conformant Markdown processor as standard]]></title>
    <link href="http://dougalstanton.github.com/blog/2013/02/22/non-conformant-markdown-processor/"/>
    <updated>2013-02-22T12:40:00+00:00</updated>
    <id>http://dougalstanton.github.com/blog/2013/02/22/non-conformant-markdown-processor</id>
    <content type="html"><![CDATA[<p>I have just discovered, while adding an <a href="/about">about page</a> to this blog, that the standard Markdown processor used by Octopress (called RDiscount) doesn't conform to the Markdown standard. I mean in particular the reference-style links, which I happen to be quite fond of. You should be able to include <code>&lt;</code> and <code>&gt;</code> characters around URLs like so:</p>

<p><code>
</code></p>

<p>Instead of stripping out these optional characters RDiscount passes them on and they get translated into HTML entities (<code>&amp;lt;</code>) which causes further steps in the chain to choke. Ultimately you end up with a very broken web page.</p>

<p>A simple <code>An [example link] [eg].</code> turns into the following mess --- oh dear.</p>

<p>``` html</p>

<p>An <a href="&lt;http://www.example.com>&#8221; title=&#8221;The canonical example page&#8221;>example link</a>.</p>


<p>```</p>

<p>Notice how the opening quote never gets closed? The rest of that paragraph is the <code>href</code> for that link! So I will have to learn to omit the angle brackets:</p>

<p><code>
</code></p>

<p>PS. While writing <em>this</em> post I discovered that Pygments.rb (which does syntax highlighting for Octopress) doesn't support Markdown as a highlightable language (fair enough, can't cover everything) --- but also that it crashes if you name a language it doesn't know. Not having a good day.</p>

<p><code>
</code> markdown I will kill your highlighter
 <code>
</code></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Home-brew static analysis for fun and profit]]></title>
    <link href="http://dougalstanton.github.com/blog/2013/02/19/home-brew-static-analysis/"/>
    <updated>2013-02-19T22:28:00+00:00</updated>
    <id>http://dougalstanton.github.com/blog/2013/02/19/home-brew-static-analysis</id>
    <content type="html"><![CDATA[<p>I've had recent success with writing test tools for use in-house. We have a system which greatly abuses the C preprocessor to create (at build-time) sets of translations and default settings for different languages and particular customers.</p>

<p>Due to the limitations of anything built with the preprocessor there isn't anything even remotely like type checking. In order to truly spot any problems in the translations you have to build the binary (and potentially even <em>run</em> it). Which means we need a single binary which includes every single customer variant and linguistic complexity, which will take an age to build; or we have to build have a dozen variants to ensure that adding one thing doesn't cause mayhem.</p>

<p>And as mentioned, building it doesn't really go all the way. If the possible values for a timer are between 5 and 300 there's no way the compiler can spot that 405 or 2 are invalid. All it can check is that the <code>int</code> you've provided will happily compile when inserted into an array of <code>int</code>.</p>

<p>Thankfully most of these checks can be done easily and quickly --- very quickly --- without even compiling. The unprocessed file has some hints to translators to indicate what are valid sizes for strings, ranges for numbers and descriptions of the text. Parsing this meta-data alongside the translations gives us all the information we need to perform these checks without compiling anything.</p>

<p>Once the necessary data is parsed and the remainder discarded we can get to the interesting work. Thankfully I started this work during a period of intense development on menu strings and default options, so the translation file was suffering a lot of abuse. Finding candidate tests was as easy as looking at the most recent reasons for fixing the file.</p>

<p>Initially I ran all tests in parallel, gathering results as they went and discarding the used data. (A single pass is definitely beneficial when you've just loaded in ~100,000 lines.) This was considerably faster than running one pass for each test. The result was printed to standard out almost instantly, which was gratifying when it passed and even better when it spotted a legitimate failure.</p>

<p>But I was tempted by the option in the Jenkins CI server to pick up JUnit-style XML files and create comparison graphs of successive runs. I could have graphs of my successful tests, for free! All I would have to do is write some XML in an undocumented format, which no longer felt so appealing.</p>

<p>Thankfully a test runner called Test Framework came to the rescue, promising to write the XML for me. I converted the format and results of my tests into some structure it could understand and let it do the hard work. I got prettier command line output and XML files that Jenkins could parse, but lost one-pass testing. I can live with this; the tests still run in about six seconds which is a considerable saving on a twenty minute build.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Salvaging the Challenge]]></title>
    <link href="http://dougalstanton.github.com/blog/2013/02/17/salvaging-the-challenge/"/>
    <updated>2013-02-17T18:57:00+00:00</updated>
    <id>http://dougalstanton.github.com/blog/2013/02/17/salvaging-the-challenge</id>
    <content type="html"><![CDATA[<p>Since losing the old blog host I've been trying to reclaim what I could
from the Wayback Machine, particularly archives of Helen's blog.</p>

<p>There aren't enough entries saved to cover the whole year, even if we
ignore the entries which aren't part of the challenge. Which basically
means we've lost the record of the challenge and frankly I'm gutted by
this state of affairs.</p>

<p>I have 67 saved entries and there should be about 190, though some of
those will be links to the same article. This is still a lot of missing
posts.</p>

<p>At this point I'm beginning to wonder if saving those 67 is worth it.
I'm quite demoralised.</p>
]]></content>
  </entry>
  
</feed>
